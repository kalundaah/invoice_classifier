{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7bk-v0gftmm"
      },
      "source": [
        "# PDF PROCESSING MODEL.\n",
        "\n",
        "This model is generated to extract data from invoice pdfs. This data should entail the recipient name, address etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQH-y2mw4rb4"
      },
      "source": [
        "# Collecting the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjpL2CXwgG6I"
      },
      "source": [
        "## Installing dependencies and importing them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6H_1P2Gfr2C",
        "outputId": "dca61fcd-9907-4592-ce15-9252da8409cf"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "# !pip install pandas\n",
        "# !pip install numpy\n",
        "# !pip install opencv-python\n",
        "# !pip install pytesseract\n",
        "# # !sudo apt-get install tesseract-ocr\n",
        "# # !sudo apt-get install poppler-utils\n",
        "# !pip install scikit-learn\n",
        "# !pip install -U spacy\n",
        "# !python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IphyNpz-gojs"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.cluster import KMeans\n",
        "# import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import pytesseract\n",
        "# from PyPDF2 import PdfReader\n",
        "# from pdf2image import convert_from_path\n",
        "# import pdfplumber\n",
        "import spacy\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3MFEPAUKLEM"
      },
      "source": [
        "## Loading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtELLWdn1Etl"
      },
      "outputs": [],
      "source": [
        "# model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZS4OFIlhgKj"
      },
      "source": [
        "## Pick the CSV and save it to a df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLfblW9Ji6SJ",
        "outputId": "dcb69c70-364e-418d-84ad-8ad8cf83ec1b"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive',force_remount=True)\n",
        "\n",
        "# file_path = '/content/drive/My Drive/data/data.csv'\n",
        "\n",
        "# with open(file_path, 'r') as file:\n",
        "#     content = file.read()\n",
        "\n",
        "# # Display the file content\n",
        "# print(content)\n",
        "\n",
        "# labeldf = pd.read_csv(\"/content/drive/My Drive/data/data.csv\")\n",
        "# labeldf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amLlkuGojlS4"
      },
      "source": [
        "## Pick the images first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3NL4oZ8jpdI"
      },
      "outputs": [],
      "source": [
        "# # Define the directory path where your invoice images are located\n",
        "# directory_path = '/content/drive/My Drive/data/'  # Replace with the actual directory path\n",
        "\n",
        "# # Define a list of image file extensions (add more if needed)\n",
        "# image_extensions = ['*.jpg', '*.jpeg', '*.png']\n",
        "\n",
        "# # Initialize lists to store image data and labels\n",
        "# image_data = []\n",
        "\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# i = 0\n",
        "# while i < 1301:\n",
        "#   for extension in image_extensions:\n",
        "#       pattern = os.path.join(directory_path, extension)\n",
        "#       image_files = glob.glob(pattern)\n",
        "#       for img_path in image_files:\n",
        "#         full_name = os.path.basename(img_path)\n",
        "#         file_name = os.path.splitext(full_name)\n",
        "#         # Load image\n",
        "#         img = cv2.imread(img_path)\n",
        "\n",
        "#         # Convert image to grayscale\n",
        "#         gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "#         # Apply threshold to convert to binary image\n",
        "#         threshold_img = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
        "\n",
        "#         # Pass the image through pytesseract\n",
        "#         text = pytesseract.image_to_string(threshold_img)\n",
        "\n",
        "#         # Prepare the data for training by tokenizing it\n",
        "#         tokens = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "#         # Obtain BERT embeddings\n",
        "#         with torch.no_grad():\n",
        "#             outputs = model(**tokens)\n",
        "\n",
        "#         # The outputs will contain the embeddings (outputs.last_hidden_state)\n",
        "#         embeddings = outputs.last_hidden_state\n",
        "\n",
        "#         max_sequence_length = 281  # Adjust as needed\n",
        "#         if embeddings.shape[1] > max_sequence_length:\n",
        "#           embeddings = embeddings[:, :max_sequence_length]\n",
        "#         elif embeddings.shape[1] < max_sequence_length:\n",
        "#           padding = torch.zeros((embeddings.shape[0], max_sequence_length - embeddings.shape[1], embeddings.shape[2]))\n",
        "#           embeddings = torch.cat((embeddings, padding), dim=1)\n",
        "\n",
        "#         name = int(file_name[0])\n",
        "#         # Print the extracted text\n",
        "#         image_data.append([name,text,tokens,embeddings])\n",
        "#         print('image', i)\n",
        "#         i = i + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNT32VIgppNq"
      },
      "source": [
        "## Save to a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_kjFKehprx5"
      },
      "outputs": [],
      "source": [
        "# textdf = pd.DataFrame(image_data,columns=['imageid','text','tokens','embeddings'])  # Store your image data here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWBFBGv0p4-A"
      },
      "source": [
        "## Print the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0U3tQ7g5p7nD"
      },
      "outputs": [],
      "source": [
        "# textdf.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-CJS7Uh0spl"
      },
      "source": [
        "## Pick up the CSV and merge it with the text dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZGHhefe00yd"
      },
      "outputs": [],
      "source": [
        "# fulldf = textdf.merge(labeldf, left_on='imageid', right_on='id', how='outer')\n",
        "# fulldf = fulldf.drop(columns=['id'])\n",
        "\n",
        "# fulldf.to_csv('/content/drive/My Drive/data/fullold.csv', index=False)\n",
        "# fulldf.head(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heohpr5A0-vG"
      },
      "source": [
        "## Drop unfilled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "x4YHYZ6B1BMV",
        "outputId": "88992f6f-7799-4d5d-eac4-aa7b1d1f176d"
      },
      "outputs": [],
      "source": [
        "# fulldf.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yShFFDS716hj"
      },
      "source": [
        "# Model implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz80FYqn5Ile"
      },
      "source": [
        "## importing the csv data for the tokenized photos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pynOwGGOEGNE"
      },
      "outputs": [],
      "source": [
        "# Data of different pages that constitute my training data\n",
        "imported = pd.read_csv(\"./files/fullnew.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LZlDNtYcYih"
      },
      "source": [
        "## initializing annotation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6TyKfB5cakL"
      },
      "outputs": [],
      "source": [
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChUCpCCVefOX"
      },
      "outputs": [],
      "source": [
        "def annotate_ner(text):\n",
        "    # Process the text with spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Initialize a list to store annotations\n",
        "    annotations = []\n",
        "\n",
        "    # Iterate through entities in the processed text\n",
        "    for ent in doc.ents:\n",
        "        annotations.append({\n",
        "            \"start\": ent.start_char,\n",
        "            \"end\": ent.end_char,\n",
        "            \"label\": ent.label_,\n",
        "            \"text\": ent.text\n",
        "        })\n",
        "\n",
        "    return annotations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8tgCzRhmX6f"
      },
      "source": [
        "## Dropping NaN rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCL0btjbgOD1",
        "outputId": "9a74d268-9f15-40ae-e148-8bbc2635989b"
      },
      "outputs": [],
      "source": [
        "print(imported.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMPxWUI-gK6m",
        "outputId": "5c8f55e9-ecfd-4550-c515-e64844c98264"
      },
      "outputs": [],
      "source": [
        "imported.dropna(inplace=True)\n",
        "\n",
        "print(imported.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCAm9OBXe0wm",
        "outputId": "b2b89cda-8b62-40b2-9e73-c6de17248a37"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(len(imported))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIVraxFfmdBv"
      },
      "source": [
        "## Forming annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJcSOOcceg66"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "annot_list = []\n",
        "while i < len(imported):\n",
        "  text = imported['text'][i]\n",
        "\n",
        "  # Perform NER annotation\n",
        "  annotations = annotate_ner(text)\n",
        "\n",
        "  # Print the annotations\n",
        "  annot_list.append([i+1,annotations])\n",
        "  print(f\"image {i+1}\")\n",
        "  i = i+1\n",
        "\n",
        "annotations = pd.DataFrame(annot_list,columns=['id','annotations'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4Qedmvqg6x-"
      },
      "outputs": [],
      "source": [
        "imported = imported.merge(annotations, left_on='imageid', right_on='id', how='outer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YHoGVlBhu5n"
      },
      "outputs": [],
      "source": [
        "imported.drop('id', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CooxfTAgiKfk",
        "outputId": "e45797bd-d928-415b-ce04-9a551ed17bc1"
      },
      "outputs": [],
      "source": [
        "imported.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNf5eB6jmgQ-"
      },
      "source": [
        "## spaCy tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bS79TiOKiolS"
      },
      "outputs": [],
      "source": [
        "spacytokens_list = []\n",
        "i = 0\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "while i < len(imported):\n",
        "  # Text to be tokenized\n",
        "  text = imported['text'][i]\n",
        "\n",
        "  # tokens\n",
        "  doc = nlp(text)\n",
        "\n",
        "  # Access tokens in the processed text\n",
        "  tokens = [token.text for token in doc]\n",
        "\n",
        "  # Print the annotations\n",
        "  spacytokens_list.append([i+1,tokens])\n",
        "  print(f\"image {i+1}\")\n",
        "  i = i+1\n",
        "\n",
        "spacytokens = pd.DataFrame(spacytokens_list,columns=['id','spacytokens'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqNlhiq9kn6h"
      },
      "outputs": [],
      "source": [
        "imported = imported.merge(spacytokens, left_on='imageid', right_on='id', how='outer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_PHzFF-kqM7"
      },
      "outputs": [],
      "source": [
        "imported.drop('id', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gd4ELQVQikC0"
      },
      "outputs": [],
      "source": [
        "imported.to_csv('./files/fullannotnew.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwFcN-RNhKVG"
      },
      "outputs": [],
      "source": [
        "  # tokenized = imported['spacytokens'][i]\n",
        "  # my_string = \" \".join(tokenized)\n",
        "  # print(my_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMkBA2fAmjRH"
      },
      "source": [
        "## Entity creations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHDRBulgmDqg"
      },
      "outputs": [],
      "source": [
        "entity_list = []\n",
        "i=0\n",
        "while i < len(imported):\n",
        "  tokenized = imported['spacytokens'][i]\n",
        "  string = \" \".join(tokenized)\n",
        "  # Process the tokenized text with spaCy\n",
        "  doc = nlp(string)\n",
        "\n",
        "  # Access named entities in the processed text\n",
        "  for ent in doc.ents:\n",
        "    entity_list.append([i+1,ent.text,ent.label_])\n",
        "\n",
        "  print(f'image {i+1}')\n",
        "  i=i+1\n",
        "\n",
        "spacyentities = pd.DataFrame(entity_list,columns=['id','entity_text','entity_label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58XPKTWtonTv",
        "outputId": "dda16752-af24-44c9-8279-63a40bce875d"
      },
      "outputs": [],
      "source": [
        "spacyentities['id'][40000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grouping the labels and entities from the results according to the id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PI0wDrGx3p5c"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Group by 'ID' and aggregate 'Label' and 'Title' as a list of dictionaries\n",
        "result = spacyentities.groupby('id').apply(lambda x: x[['entity_label', 'entity_text']].to_dict(orient='records')).reset_index(name='Data')\n",
        "\n",
        "# Initialize an empty list to store the output dictionaries\n",
        "output_list = []\n",
        "\n",
        "# Create dictionaries for each 'ID' containing arrays of labels and titles\n",
        "for index, row in result.iterrows():\n",
        "    id_dict = {'id': row['id'], 'Data': row['Data']}\n",
        "    output_list.append(id_dict)\n",
        "\n",
        "# # Now, output_list contains dictionaries with dimensions 1x1\n",
        "# for item in output_list:\n",
        "#     print(item)\n",
        "\n",
        "# entities = pd.DataFrame(output_list,columns=['id','entities'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhpCOsPPzZ2I",
        "outputId": "f7c6404d-6abc-45f6-cbc0-0627c6908496"
      },
      "outputs": [],
      "source": [
        "print(len(output_list))\n",
        "# Now, output_list contains dictionaries with dimensions 1x1\n",
        "for item in output_list:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ChatGPT including"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## importing new list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "newimported = pd.read_csv('./files/fullannotnew.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "newimported.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Include chatgpt in deduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, openai, shutil, json\n",
        "from llama_index import Document, VectorStoreIndex, LLMPredictor, Prompt\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from dotenv import load_dotenv\n",
        "from typing import Any, List\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "os.environ['USE_TORCH'] = '1'\n",
        "os.environ['OPENAI_API_KEY'] = str(os.getenv('OPENAI_API_KEY'))\n",
        "openai.api_key = str(os.getenv('OPENAI_API_KEY'))\n",
        "openai.api_endpoint = \"https://api.openai.com/v1\"\n",
        "\n",
        "TEMPLATE = (\n",
        "    \"You are a helpful assistant that extracts specific information from unstructured data. That data is provided below\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"{context_str}\"\n",
        "    \"\\n---------------------\\n\"\n",
        "    \"Given this information, please answer the question with precision: {query_str}\\n\"\n",
        ")\n",
        "QA_TEMPLATE = Prompt(TEMPLATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(os.getenv('OPENAI_API_KEY'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_data(text:str, questions:List[str]) -> str:\n",
        "    document = Document(text=text)\n",
        "\n",
        "    llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name='gpt-4'))\n",
        "    index = VectorStoreIndex.from_documents([document], llm_predictor=llm_predictor)\n",
        "\n",
        "    query_engine = index.as_query_engine(text_qa_template=QA_TEMPLATE)\n",
        "\n",
        "    result_string = \"\"\n",
        "    for q in questions:\n",
        "        result_string += '---------------\\n'\n",
        "        # result_string += 'QUESTION:\\n'\n",
        "        # result_string += q + '\\n'\n",
        "        # result_string += 'ANSWER:\\n'\n",
        "        result_string += query_engine.query(q).response + '\\n'\n",
        "\n",
        "    return result_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lambda_handler(text):\n",
        "    questions_string = [\n",
        "        'what is the name of the recipient company',\n",
        "        'what is the address of the recipient company',\n",
        "        'what is the invoice number',\n",
        "        'what is the invoice date',\n",
        "        'what is the due date of the invoice',\n",
        "        'what is the due balance to be paid',\n",
        "    ]\n",
        "    questions = questions_string if isinstance(questions_string, list) else questions_string.split(',')\n",
        "\n",
        "    result = extract_data(text, questions)\n",
        "\n",
        "    return(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lambda_handler(newimported['text'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "key = \"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\"\n",
        "import openai\n",
        "\n",
        "# Set your API key\n",
        "openai.api_key = key\n",
        "\n",
        "# Test the API connection by making a simple request with an engine specified\n",
        "try:\n",
        "    response = openai.Completion.create(\n",
        "        prompt=\"Test connection\",\n",
        "        engine=\"text-davinci-002\"  # Specify the engine or model you want to use\n",
        "    )\n",
        "    if response.choices:\n",
        "        print(\"API connection test successful.\")\n",
        "        print(\"Response:\", response.choices[0].text)\n",
        "    else:\n",
        "        print(\"API request was successful, but the response is empty.\")\n",
        "except Exception as e:\n",
        "    print(\"API connection test failed. Error:\", str(e))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Unnecessary code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCztDruV54KY"
      },
      "source": [
        "## changing the columns to objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8ol6d-33Y_c"
      },
      "outputs": [],
      "source": [
        "# imported['imageid'] = imported['imageid'].apply(lambda x: np.array([x]))\n",
        "# imported['embeddings'] = imported['embeddings'].apply(lambda x: np.array([x]))\n",
        "\n",
        "# imported.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7x5X0GezaSG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djGic9nU59fx"
      },
      "source": [
        "## Confirming the tensor sizes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvGSG_rOVD1Q"
      },
      "outputs": [],
      "source": [
        "# for i, tensor in enumerate(fulldf['embeddings'].tolist()):\n",
        "#     print(f\"Tensor {i + 1} size: {tensor.size()}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0CkcoOVIbN2"
      },
      "outputs": [],
      "source": [
        "# fulldf.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFuvUsvDohZ6"
      },
      "outputs": [],
      "source": [
        "# Saving all the data to a csv\n",
        "# fulldf.to_csv('/content/drive/My Drive/data/full.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRVjMPPHLqy6"
      },
      "source": [
        "## Drops the NaN values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHrbwFs_Ulwf"
      },
      "outputs": [],
      "source": [
        "# fulldf.dropna(inplace=True)\n",
        "\n",
        "# print(fulldf.isna().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnK3Lcwc0Zk1"
      },
      "outputs": [],
      "source": [
        "# for i, tensor in enumerate(fulldf['embeddings'].tolist()):\n",
        "#     print(f\"Tensor {i + 1} size: {tensor.size()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw9gFd4BcsQ_"
      },
      "source": [
        "## Decreasing/adjusting the tensor size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcw1lXwNi24h"
      },
      "outputs": [],
      "source": [
        "# print(len(fulldf))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zH0MaJukEGaM"
      },
      "source": [
        "## Importing a fully filled data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t03SWqG23sF4"
      },
      "outputs": [],
      "source": [
        "# fulldf['imageid'] = fulldf['imageid'].apply(lambda x: np.array([x]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQfSX-Fq8yqc"
      },
      "outputs": [],
      "source": [
        "# # Assuming fulldf is your DataFrame\n",
        "# fulldf_array = fulldf.to_numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUSzPV4ThUzB"
      },
      "outputs": [],
      "source": [
        "# from sklearn.cluster import KMeans\n",
        "# import numpy as np\n",
        "\n",
        "# # Flatten the 3D embeddings into 2D\n",
        "# embeddings_flat = [embedding.view(-1, embedding.shape[-1]).numpy() for embedding in fulldf_array['embeddings']]\n",
        "\n",
        "# # Stack the flattened embeddings into a single 2D array\n",
        "# embeddings_2d = np.vstack(embeddings_flat)\n",
        "\n",
        "# # Perform K-Means clustering\n",
        "# NUM_CLUSTERS = 6  # You should specify the number of clusters\n",
        "# kmeans = KMeans(n_clusters=NUM_CLUSTERS)\n",
        "# fulldf_array['cluster_labels'] = kmeans.fit_predict(embeddings_2d)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAfntlL6-tDy"
      },
      "outputs": [],
      "source": [
        "# from sklearn.cluster import KMeans\n",
        "# import numpy as np\n",
        "\n",
        "# # Assuming fulldf_array is your NumPy array\n",
        "# # Assuming the 'embeddings' column is in the third position (index 2)\n",
        "# embeddings_flat = [embedding.view(-1, embedding.shape[-1]).numpy() for embedding in fulldf_array[:, 2]]\n",
        "\n",
        "# # Stack the flattened embeddings into a single 2D array\n",
        "# embeddings_2d = np.vstack(embeddings_flat)\n",
        "\n",
        "# # Perform K-Means clustering\n",
        "# NUM_CLUSTERS = 6  # You should specify the number of clusters\n",
        "# kmeans = KMeans(n_clusters=NUM_CLUSTERS)\n",
        "# cluster_labels = kmeans.fit_predict(embeddings_2d)\n",
        "\n",
        "# # Add the cluster labels back to the original NumPy array\n",
        "# fulldf_array_with_clusters = np.column_stack((fulldf_array, cluster_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5K7Ik9l_ALb"
      },
      "outputs": [],
      "source": [
        "# from sklearn.cluster import KMeans\n",
        "# import numpy as np\n",
        "\n",
        "# embedd_array = np.array(fulldf['embeddings'])\n",
        "# embeddings_flat = [embedding.view(-1, embedding.shape[-1]).numpy() for embedding in fulldf['embedding']]\n",
        "# # Extract the 'embeddings' column from the Pandas DataFrame and convert it to a NumPy array\n",
        "# # embeddings_array = np.array([embedding.view(-1, embedding.shape[-1]).numpy() for embedding in fulldf['embeddings']])\n",
        "\n",
        "# # Perform K-Means clustering on the embeddings_array\n",
        "# NUM_CLUSTERS = 6  # You should specify the number of clusters\n",
        "# kmeans = KMeans(n_clusters=NUM_CLUSTERS)\n",
        "# cluster_labels = kmeans.fit_predict(embedd_array)\n",
        "\n",
        "# # Add the cluster labels as a new column in the Pandas DataFrame\n",
        "# fulldf['cluster_labels'] = cluster_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0TUI3rG3CZ8"
      },
      "outputs": [],
      "source": [
        "# fulldf['imageid'] = fulldf['imageid'].apply(lambda x: np.array([x]))\n",
        "\n",
        "# # Flatten the 3D embeddings into 2D\n",
        "# embeddings_flat = [embedding.view(-1, embedding.shape[-1]).numpy() for embedding in fulldf['embeddings']]\n",
        "\n",
        "# # Stack the flattened embeddings into a single 2D array\n",
        "# embeddings_2d = np.vstack(embeddings_flat)\n",
        "\n",
        "# # Perform K-Means clustering\n",
        "# NUM_CLUSTERS = 6  # You should specify the number of clusters\n",
        "# kmeans = KMeans(n_clusters=NUM_CLUSTERS)\n",
        "# fulldf['cluster_labels'] = kmeans.fit_predict(embeddings_2d)\n",
        "\n",
        "# # Now, 'fulldf' contains an additional column 'cluster_labels' with cluster assignments\n",
        "# # You can save 'fulldf' to a CSV file if needed\n",
        "# fulldf.to_csv('/content/drive/My Drive/data/full_with_clusters.csv', index=False)\n",
        "\n",
        "# # Print the first few rows of 'fulldf' to see the cluster assignments\n",
        "# print(fulldf.head())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
