{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7bk-v0gftmm"
      },
      "source": [
        "# PDF PROCESSING MODEL.\n",
        "\n",
        "This model is generated to extract data from invoice pdfs. This data should entail the recipient name, address etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQH-y2mw4rb4"
      },
      "source": [
        "# Collecting the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjpL2CXwgG6I"
      },
      "source": [
        "## Installing dependencies and importing them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6H_1P2Gfr2C",
        "outputId": "dca61fcd-9907-4592-ce15-9252da8409cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: transformers in /home/kalundaah/.local/lib/python3.11/site-packages (4.33.1)\n",
            "Requirement already satisfied: filelock in /home/kalundaah/.local/lib/python3.11/site-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /home/kalundaah/.local/lib/python3.11/site-packages (from transformers) (0.17.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/kalundaah/.local/lib/python3.11/site-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/kalundaah/.local/lib/python3.11/site-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/lib64/python3.11/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/lib64/python3.11/site-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/lib/python3.11/site-packages (from transformers) (2.28.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/kalundaah/.local/lib/python3.11/site-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /home/kalundaah/.local/lib/python3.11/site-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/kalundaah/.local/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /home/kalundaah/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/kalundaah/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/lib/python3.11/site-packages (from requests->transformers) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pandas in /home/kalundaah/.local/lib/python3.11/site-packages (2.1.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /home/kalundaah/.local/lib/python3.11/site-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/kalundaah/.local/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/kalundaah/.local/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /home/kalundaah/.local/lib/python3.11/site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /home/kalundaah/.local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: numpy in /home/kalundaah/.local/lib/python3.11/site-packages (1.25.2)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: opencv-python in /home/kalundaah/.local/lib/python3.11/site-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /home/kalundaah/.local/lib/python3.11/site-packages (from opencv-python) (1.25.2)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pytesseract in /home/kalundaah/.local/lib/python3.11/site-packages (0.3.10)\n",
            "Requirement already satisfied: packaging>=21.3 in /home/kalundaah/.local/lib/python3.11/site-packages (from pytesseract) (23.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/lib64/python3.11/site-packages (from pytesseract) (9.4.0)\n",
            "Error: This command has to be run with superuser privileges (under the root user on most systems).\n",
            "Error: This command has to be run with superuser privileges (under the root user on most systems).\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m583.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /home/kalundaah/.local/lib/python3.11/site-packages (from scikit-learn) (1.25.2)\n",
            "Collecting scipy>=1.5.0\n",
            "  Downloading scipy-1.11.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hCollecting joblib>=1.1.1\n",
            "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
            "Successfully installed joblib-1.3.2 scikit-learn-1.3.0 scipy-1.11.2 threadpoolctl-3.2.0\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
            "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.11\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0\n",
            "  Downloading murmurhash-1.0.9-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2\n",
            "  Downloading cymem-2.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2\n",
            "  Downloading preshed-3.0.8-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (122 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.1/122.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting thinc<8.2.0,>=8.1.8\n",
            "  Downloading thinc-8.1.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (917 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m917.4/917.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1\n",
            "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3\n",
            "  Downloading srsly-2.4.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (490 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.8/490.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.9-py3-none-any.whl (17 kB)\n",
            "Collecting typer<0.10.0,>=0.3.0\n",
            "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m857.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting pathy>=0.10.0\n",
            "  Downloading pathy-0.10.2-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting smart-open<7.0.0,>=5.2.1\n",
            "  Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m990.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/kalundaah/.local/lib/python3.11/site-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /home/kalundaah/.local/lib/python3.11/site-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3.11/site-packages (from spacy) (2.28.2)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
            "  Downloading pydantic-2.3.0-py3-none-any.whl (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting jinja2\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/lib/python3.11/site-packages (from spacy) (65.5.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/kalundaah/.local/lib/python3.11/site-packages (from spacy) (23.1)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting annotated-types>=0.4.0\n",
            "  Downloading annotated_types-0.5.0-py3-none-any.whl (11 kB)\n",
            "Collecting pydantic-core==2.6.3\n",
            "  Downloading pydantic_core-2.6.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.6.1 in /home/kalundaah/.local/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
            "Collecting blis<0.8.0,>=0.7.8\n",
            "  Downloading blis-0.7.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
            "  Downloading confection-0.1.3-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib64/python3.11/site-packages (from jinja2->spacy) (2.1.2)\n",
            "Installing collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, smart-open, pydantic-core, murmurhash, langcodes, jinja2, catalogue, blis, annotated-types, srsly, pydantic, preshed, pathy, confection, thinc, spacy\n",
            "Successfully installed annotated-types-0.5.0 blis-0.7.10 catalogue-2.0.9 confection-0.1.3 cymem-2.0.7 jinja2-3.1.2 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.2 preshed-3.0.8 pydantic-2.3.0 pydantic-core-2.6.3 smart-open-6.4.0 spacy-3.6.1 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.7 thinc-8.1.12 typer-0.9.0 wasabi-1.1.2\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /home/kalundaah/.local/lib/python3.11/site-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/kalundaah/.local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/kalundaah/.local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/kalundaah/.local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/kalundaah/.local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/kalundaah/.local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/kalundaah/.local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/kalundaah/.local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/kalundaah/.local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/kalundaah/.local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/kalundaah/.local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /home/kalundaah/.local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/kalundaah/.local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/kalundaah/.local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /home/kalundaah/.local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.25.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.28.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/kalundaah/.local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.3.0)\n",
            "Requirement already satisfied: jinja2 in /home/kalundaah/.local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (65.5.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/kalundaah/.local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/kalundaah/.local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/kalundaah/.local/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.3 in /home/kalundaah/.local/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.6.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /home/kalundaah/.local/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.26.16)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/kalundaah/.local/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/kalundaah/.local/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib64/python3.11/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.2)\n",
            "Installing collected packages: en-core-web-sm\n",
            "Successfully installed en-core-web-sm-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install opencv-python\n",
        "!pip install pytesseract\n",
        "!dnf install tesseract-ocr\n",
        "!dnf install poppler-utils\n",
        "!pip install scikit-learn\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IphyNpz-gojs"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.cluster import KMeans\n",
        "# import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import pytesseract\n",
        "# from PyPDF2 import PdfReader\n",
        "# from pdf2image import convert_from_path\n",
        "# import pdfplumber\n",
        "import spacy\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3MFEPAUKLEM"
      },
      "source": [
        "## Loading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtELLWdn1Etl"
      },
      "outputs": [],
      "source": [
        "# model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZS4OFIlhgKj"
      },
      "source": [
        "## Pick the CSV and save it to a df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLfblW9Ji6SJ",
        "outputId": "dcb69c70-364e-418d-84ad-8ad8cf83ec1b"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive',force_remount=True)\n",
        "\n",
        "# file_path = '/content/drive/My Drive/data/data.csv'\n",
        "\n",
        "# with open(file_path, 'r') as file:\n",
        "#     content = file.read()\n",
        "\n",
        "# # Display the file content\n",
        "# print(content)\n",
        "\n",
        "# labeldf = pd.read_csv(\"/content/drive/My Drive/data/data.csv\")\n",
        "# labeldf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amLlkuGojlS4"
      },
      "source": [
        "## Pick the images first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3NL4oZ8jpdI"
      },
      "outputs": [],
      "source": [
        "# # Define the directory path where your invoice images are located\n",
        "# directory_path = '/content/drive/My Drive/data/'  # Replace with the actual directory path\n",
        "\n",
        "# # Define a list of image file extensions (add more if needed)\n",
        "# image_extensions = ['*.jpg', '*.jpeg', '*.png']\n",
        "\n",
        "# # Initialize lists to store image data and labels\n",
        "# image_data = []\n",
        "\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# i = 0\n",
        "# while i < 1301:\n",
        "#   for extension in image_extensions:\n",
        "#       pattern = os.path.join(directory_path, extension)\n",
        "#       image_files = glob.glob(pattern)\n",
        "#       for img_path in image_files:\n",
        "#         full_name = os.path.basename(img_path)\n",
        "#         file_name = os.path.splitext(full_name)\n",
        "#         # Load image\n",
        "#         img = cv2.imread(img_path)\n",
        "\n",
        "#         # Convert image to grayscale\n",
        "#         gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "#         # Apply threshold to convert to binary image\n",
        "#         threshold_img = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
        "\n",
        "#         # Pass the image through pytesseract\n",
        "#         text = pytesseract.image_to_string(threshold_img)\n",
        "\n",
        "#         # Prepare the data for training by tokenizing it\n",
        "#         tokens = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "#         # Obtain BERT embeddings\n",
        "#         with torch.no_grad():\n",
        "#             outputs = model(**tokens)\n",
        "\n",
        "#         # The outputs will contain the embeddings (outputs.last_hidden_state)\n",
        "#         embeddings = outputs.last_hidden_state\n",
        "\n",
        "#         max_sequence_length = 281  # Adjust as needed\n",
        "#         if embeddings.shape[1] > max_sequence_length:\n",
        "#           embeddings = embeddings[:, :max_sequence_length]\n",
        "#         elif embeddings.shape[1] < max_sequence_length:\n",
        "#           padding = torch.zeros((embeddings.shape[0], max_sequence_length - embeddings.shape[1], embeddings.shape[2]))\n",
        "#           embeddings = torch.cat((embeddings, padding), dim=1)\n",
        "\n",
        "#         name = int(file_name[0])\n",
        "#         # Print the extracted text\n",
        "#         image_data.append([name,text,tokens,embeddings])\n",
        "#         print('image', i)\n",
        "#         i = i + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNT32VIgppNq"
      },
      "source": [
        "## Save to a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_kjFKehprx5"
      },
      "outputs": [],
      "source": [
        "# textdf = pd.DataFrame(image_data,columns=['imageid','text','tokens','embeddings'])  # Store your image data here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWBFBGv0p4-A"
      },
      "source": [
        "## Print the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0U3tQ7g5p7nD"
      },
      "outputs": [],
      "source": [
        "# textdf.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-CJS7Uh0spl"
      },
      "source": [
        "## Pick up the CSV and merge it with the text dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZGHhefe00yd"
      },
      "outputs": [],
      "source": [
        "# fulldf = textdf.merge(labeldf, left_on='imageid', right_on='id', how='outer')\n",
        "# fulldf = fulldf.drop(columns=['id'])\n",
        "\n",
        "# fulldf.to_csv('/content/drive/My Drive/data/fullold.csv', index=False)\n",
        "# fulldf.head(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heohpr5A0-vG"
      },
      "source": [
        "## Drop unfilled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "x4YHYZ6B1BMV",
        "outputId": "88992f6f-7799-4d5d-eac4-aa7b1d1f176d"
      },
      "outputs": [],
      "source": [
        "# fulldf.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yShFFDS716hj"
      },
      "source": [
        "# Model implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz80FYqn5Ile"
      },
      "source": [
        "## importing the csv data for the tokenized photos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pynOwGGOEGNE"
      },
      "outputs": [],
      "source": [
        "# Data of different pages that constitute my training data\n",
        "imported = pd.read_csv(\"./files/fullnew.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LZlDNtYcYih"
      },
      "source": [
        "## initializing annotation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6TyKfB5cakL"
      },
      "outputs": [],
      "source": [
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChUCpCCVefOX"
      },
      "outputs": [],
      "source": [
        "def annotate_ner(text):\n",
        "    # Process the text with spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Initialize a list to store annotations\n",
        "    annotations = []\n",
        "\n",
        "    # Iterate through entities in the processed text\n",
        "    for ent in doc.ents:\n",
        "        annotations.append({\n",
        "            \"start\": ent.start_char,\n",
        "            \"end\": ent.end_char,\n",
        "            \"label\": ent.label_,\n",
        "            \"text\": ent.text\n",
        "        })\n",
        "\n",
        "    return annotations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8tgCzRhmX6f"
      },
      "source": [
        "## Dropping NaN rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCL0btjbgOD1",
        "outputId": "9a74d268-9f15-40ae-e148-8bbc2635989b"
      },
      "outputs": [],
      "source": [
        "print(imported.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMPxWUI-gK6m",
        "outputId": "5c8f55e9-ecfd-4550-c515-e64844c98264"
      },
      "outputs": [],
      "source": [
        "imported.dropna(inplace=True)\n",
        "\n",
        "print(imported.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCAm9OBXe0wm",
        "outputId": "b2b89cda-8b62-40b2-9e73-c6de17248a37"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(len(imported))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIVraxFfmdBv"
      },
      "source": [
        "## Forming annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJcSOOcceg66"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "annot_list = []\n",
        "while i < len(imported):\n",
        "  text = imported['text'][i]\n",
        "\n",
        "  # Perform NER annotation\n",
        "  annotations = annotate_ner(text)\n",
        "\n",
        "  # Print the annotations\n",
        "  annot_list.append([i+1,annotations])\n",
        "  print(f\"image {i+1}\")\n",
        "  i = i+1\n",
        "\n",
        "annotations = pd.DataFrame(annot_list,columns=['id','annotations'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4Qedmvqg6x-"
      },
      "outputs": [],
      "source": [
        "imported = imported.merge(annotations, left_on='imageid', right_on='id', how='outer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YHoGVlBhu5n"
      },
      "outputs": [],
      "source": [
        "imported.drop('id', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CooxfTAgiKfk",
        "outputId": "e45797bd-d928-415b-ce04-9a551ed17bc1"
      },
      "outputs": [],
      "source": [
        "imported.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNf5eB6jmgQ-"
      },
      "source": [
        "## spaCy tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bS79TiOKiolS"
      },
      "outputs": [],
      "source": [
        "spacytokens_list = []\n",
        "i = 0\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "while i < len(imported):\n",
        "  # Text to be tokenized\n",
        "  text = imported['text'][i]\n",
        "\n",
        "  # tokens\n",
        "  doc = nlp(text)\n",
        "\n",
        "  # Access tokens in the processed text\n",
        "  tokens = [token.text for token in doc]\n",
        "\n",
        "  # Print the annotations\n",
        "  spacytokens_list.append([i+1,tokens])\n",
        "  print(f\"image {i+1}\")\n",
        "  i = i+1\n",
        "\n",
        "spacytokens = pd.DataFrame(spacytokens_list,columns=['id','spacytokens'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqNlhiq9kn6h"
      },
      "outputs": [],
      "source": [
        "imported = imported.merge(spacytokens, left_on='imageid', right_on='id', how='outer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_PHzFF-kqM7"
      },
      "outputs": [],
      "source": [
        "imported.drop('id', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gd4ELQVQikC0"
      },
      "outputs": [],
      "source": [
        "imported.to_csv('./files/fullannotnew.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwFcN-RNhKVG"
      },
      "outputs": [],
      "source": [
        "  # tokenized = imported['spacytokens'][i]\n",
        "  # my_string = \" \".join(tokenized)\n",
        "  # print(my_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMkBA2fAmjRH"
      },
      "source": [
        "## Entity creations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHDRBulgmDqg"
      },
      "outputs": [],
      "source": [
        "entity_list = []\n",
        "i=0\n",
        "while i < len(imported):\n",
        "  tokenized = imported['spacytokens'][i]\n",
        "  string = \" \".join(tokenized)\n",
        "  # Process the tokenized text with spaCy\n",
        "  doc = nlp(string)\n",
        "\n",
        "  # Access named entities in the processed text\n",
        "  for ent in doc.ents:\n",
        "    entity_list.append([i+1,ent.text,ent.label_])\n",
        "\n",
        "  print(f'image {i+1}')\n",
        "  i=i+1\n",
        "\n",
        "spacyentities = pd.DataFrame(entity_list,columns=['id','entity_text','entity_label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58XPKTWtonTv",
        "outputId": "dda16752-af24-44c9-8279-63a40bce875d"
      },
      "outputs": [],
      "source": [
        "spacyentities['id'][40000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grouping the labels and entities from the results according to the id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PI0wDrGx3p5c"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Group by 'ID' and aggregate 'Label' and 'Title' as a list of dictionaries\n",
        "result = spacyentities.groupby('id').apply(lambda x: x[['entity_label', 'entity_text']].to_dict(orient='records')).reset_index(name='Data')\n",
        "\n",
        "# Initialize an empty list to store the output dictionaries\n",
        "output_list = []\n",
        "\n",
        "# Create dictionaries for each 'ID' containing arrays of labels and titles\n",
        "for index, row in result.iterrows():\n",
        "    id_dict = {'id': row['id'], 'Data': row['Data']}\n",
        "    output_list.append(id_dict)\n",
        "\n",
        "# # Now, output_list contains dictionaries with dimensions 1x1\n",
        "# for item in output_list:\n",
        "#     print(item)\n",
        "\n",
        "# entities = pd.DataFrame(output_list,columns=['id','entities'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhpCOsPPzZ2I",
        "outputId": "f7c6404d-6abc-45f6-cbc0-0627c6908496"
      },
      "outputs": [],
      "source": [
        "print(len(output_list))\n",
        "# Now, output_list contains dictionaries with dimensions 1x1\n",
        "for item in output_list:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ChatGPT including"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## importing new list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "newimported = pd.read_csv('./files/fullannotnew.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "newimported.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Include chatgpt in deduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, openai, shutil, json\n",
        "from llama_index import Document, VectorStoreIndex, LLMPredictor, Prompt\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from dotenv import load_dotenv\n",
        "from typing import Any, List\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "os.environ['USE_TORCH'] = '1'\n",
        "os.environ['OPENAI_API_KEY'] = str(os.getenv('OPENAI_API_KEY'))\n",
        "openai.api_key = str(os.getenv('OPENAI_API_KEY'))\n",
        "openai.api_endpoint = \"https://api.openai.com/v1\"\n",
        "\n",
        "TEMPLATE = (\n",
        "    \"You are a helpful assistant that extracts specific information from unstructured data. That data is provided below\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"{context_str}\"\n",
        "    \"\\n---------------------\\n\"\n",
        "    \"Given this information, please answer the question with precision: {query_str}\\n\"\n",
        ")\n",
        "QA_TEMPLATE = Prompt(TEMPLATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(os.getenv('OPENAI_API_KEY'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_data(text:str, questions:List[str]) -> str:\n",
        "    document = Document(text=text)\n",
        "\n",
        "    llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name='gpt-4'))\n",
        "    index = VectorStoreIndex.from_documents([document], llm_predictor=llm_predictor)\n",
        "\n",
        "    query_engine = index.as_query_engine(text_qa_template=QA_TEMPLATE)\n",
        "\n",
        "    result_string = \"\"\n",
        "    for q in questions:\n",
        "        result_string += '---------------\\n'\n",
        "        # result_string += 'QUESTION:\\n'\n",
        "        # result_string += q + '\\n'\n",
        "        # result_string += 'ANSWER:\\n'\n",
        "        result_string += query_engine.query(q).response + '\\n'\n",
        "\n",
        "    return result_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lambda_handler(text):\n",
        "    questions_string = [\n",
        "        'what is the name of the recipient company',\n",
        "        'what is the address of the recipient company',\n",
        "        'what is the invoice number',\n",
        "        'what is the invoice date',\n",
        "        'what is the due date of the invoice',\n",
        "        'what is the due balance to be paid',\n",
        "    ]\n",
        "    questions = questions_string if isinstance(questions_string, list) else questions_string.split(',')\n",
        "\n",
        "    result = extract_data(text, questions)\n",
        "\n",
        "    return(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lambda_handler(newimported['text'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "key = \"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\"\n",
        "import openai\n",
        "\n",
        "# Set your API key\n",
        "openai.api_key = key\n",
        "\n",
        "# Test the API connection by making a simple request with an engine specified\n",
        "try:\n",
        "    response = openai.Completion.create(\n",
        "        prompt=\"Test connection\",\n",
        "        engine=\"text-davinci-002\"  # Specify the engine or model you want to use\n",
        "    )\n",
        "    if response.choices:\n",
        "        print(\"API connection test successful.\")\n",
        "        print(\"Response:\", response.choices[0].text)\n",
        "    else:\n",
        "        print(\"API request was successful, but the response is empty.\")\n",
        "except Exception as e:\n",
        "    print(\"API connection test failed. Error:\", str(e))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Unnecessary code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCztDruV54KY"
      },
      "source": [
        "## changing the columns to objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8ol6d-33Y_c"
      },
      "outputs": [],
      "source": [
        "# imported['imageid'] = imported['imageid'].apply(lambda x: np.array([x]))\n",
        "# imported['embeddings'] = imported['embeddings'].apply(lambda x: np.array([x]))\n",
        "\n",
        "# imported.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7x5X0GezaSG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djGic9nU59fx"
      },
      "source": [
        "## Confirming the tensor sizes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvGSG_rOVD1Q"
      },
      "outputs": [],
      "source": [
        "# for i, tensor in enumerate(fulldf['embeddings'].tolist()):\n",
        "#     print(f\"Tensor {i + 1} size: {tensor.size()}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0CkcoOVIbN2"
      },
      "outputs": [],
      "source": [
        "# fulldf.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFuvUsvDohZ6"
      },
      "outputs": [],
      "source": [
        "# Saving all the data to a csv\n",
        "# fulldf.to_csv('/content/drive/My Drive/data/full.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRVjMPPHLqy6"
      },
      "source": [
        "## Drops the NaN values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHrbwFs_Ulwf"
      },
      "outputs": [],
      "source": [
        "# fulldf.dropna(inplace=True)\n",
        "\n",
        "# print(fulldf.isna().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnK3Lcwc0Zk1"
      },
      "outputs": [],
      "source": [
        "# for i, tensor in enumerate(fulldf['embeddings'].tolist()):\n",
        "#     print(f\"Tensor {i + 1} size: {tensor.size()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw9gFd4BcsQ_"
      },
      "source": [
        "## Decreasing/adjusting the tensor size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcw1lXwNi24h"
      },
      "outputs": [],
      "source": [
        "# print(len(fulldf))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zH0MaJukEGaM"
      },
      "source": [
        "## Importing a fully filled data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t03SWqG23sF4"
      },
      "outputs": [],
      "source": [
        "# fulldf['imageid'] = fulldf['imageid'].apply(lambda x: np.array([x]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQfSX-Fq8yqc"
      },
      "outputs": [],
      "source": [
        "# # Assuming fulldf is your DataFrame\n",
        "# fulldf_array = fulldf.to_numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUSzPV4ThUzB"
      },
      "outputs": [],
      "source": [
        "# from sklearn.cluster import KMeans\n",
        "# import numpy as np\n",
        "\n",
        "# # Flatten the 3D embeddings into 2D\n",
        "# embeddings_flat = [embedding.view(-1, embedding.shape[-1]).numpy() for embedding in fulldf_array['embeddings']]\n",
        "\n",
        "# # Stack the flattened embeddings into a single 2D array\n",
        "# embeddings_2d = np.vstack(embeddings_flat)\n",
        "\n",
        "# # Perform K-Means clustering\n",
        "# NUM_CLUSTERS = 6  # You should specify the number of clusters\n",
        "# kmeans = KMeans(n_clusters=NUM_CLUSTERS)\n",
        "# fulldf_array['cluster_labels'] = kmeans.fit_predict(embeddings_2d)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAfntlL6-tDy"
      },
      "outputs": [],
      "source": [
        "# from sklearn.cluster import KMeans\n",
        "# import numpy as np\n",
        "\n",
        "# # Assuming fulldf_array is your NumPy array\n",
        "# # Assuming the 'embeddings' column is in the third position (index 2)\n",
        "# embeddings_flat = [embedding.view(-1, embedding.shape[-1]).numpy() for embedding in fulldf_array[:, 2]]\n",
        "\n",
        "# # Stack the flattened embeddings into a single 2D array\n",
        "# embeddings_2d = np.vstack(embeddings_flat)\n",
        "\n",
        "# # Perform K-Means clustering\n",
        "# NUM_CLUSTERS = 6  # You should specify the number of clusters\n",
        "# kmeans = KMeans(n_clusters=NUM_CLUSTERS)\n",
        "# cluster_labels = kmeans.fit_predict(embeddings_2d)\n",
        "\n",
        "# # Add the cluster labels back to the original NumPy array\n",
        "# fulldf_array_with_clusters = np.column_stack((fulldf_array, cluster_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5K7Ik9l_ALb"
      },
      "outputs": [],
      "source": [
        "# from sklearn.cluster import KMeans\n",
        "# import numpy as np\n",
        "\n",
        "# embedd_array = np.array(fulldf['embeddings'])\n",
        "# embeddings_flat = [embedding.view(-1, embedding.shape[-1]).numpy() for embedding in fulldf['embedding']]\n",
        "# # Extract the 'embeddings' column from the Pandas DataFrame and convert it to a NumPy array\n",
        "# # embeddings_array = np.array([embedding.view(-1, embedding.shape[-1]).numpy() for embedding in fulldf['embeddings']])\n",
        "\n",
        "# # Perform K-Means clustering on the embeddings_array\n",
        "# NUM_CLUSTERS = 6  # You should specify the number of clusters\n",
        "# kmeans = KMeans(n_clusters=NUM_CLUSTERS)\n",
        "# cluster_labels = kmeans.fit_predict(embedd_array)\n",
        "\n",
        "# # Add the cluster labels as a new column in the Pandas DataFrame\n",
        "# fulldf['cluster_labels'] = cluster_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0TUI3rG3CZ8"
      },
      "outputs": [],
      "source": [
        "# fulldf['imageid'] = fulldf['imageid'].apply(lambda x: np.array([x]))\n",
        "\n",
        "# # Flatten the 3D embeddings into 2D\n",
        "# embeddings_flat = [embedding.view(-1, embedding.shape[-1]).numpy() for embedding in fulldf['embeddings']]\n",
        "\n",
        "# # Stack the flattened embeddings into a single 2D array\n",
        "# embeddings_2d = np.vstack(embeddings_flat)\n",
        "\n",
        "# # Perform K-Means clustering\n",
        "# NUM_CLUSTERS = 6  # You should specify the number of clusters\n",
        "# kmeans = KMeans(n_clusters=NUM_CLUSTERS)\n",
        "# fulldf['cluster_labels'] = kmeans.fit_predict(embeddings_2d)\n",
        "\n",
        "# # Now, 'fulldf' contains an additional column 'cluster_labels' with cluster assignments\n",
        "# # You can save 'fulldf' to a CSV file if needed\n",
        "# fulldf.to_csv('/content/drive/My Drive/data/full_with_clusters.csv', index=False)\n",
        "\n",
        "# # Print the first few rows of 'fulldf' to see the cluster assignments\n",
        "# print(fulldf.head())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
