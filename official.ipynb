{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7bk-v0gftmm"
      },
      "source": [
        "# PDF PROCESSING MODEL.\n",
        "\n",
        "This model is generated to extract data from invoice pdfs. This data should entail the recipient name, address etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQH-y2mw4rb4"
      },
      "source": [
        "# Collecting the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjpL2CXwgG6I"
      },
      "source": [
        "## Installing dependencies and importing them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6H_1P2Gfr2C",
        "outputId": "dca61fcd-9907-4592-ce15-9252da8409cf"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "# !pip install pandas\n",
        "# !pip install numpy\n",
        "# !pip install opencv-python\n",
        "# !pip install pytesseract\n",
        "# !dnf install tesseract-ocr\n",
        "# !dnf install poppler-utils\n",
        "# !pip install scikit-learn\n",
        "# !pip install -U spacy\n",
        "# !python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZS4OFIlhgKj"
      },
      "source": [
        "## Pick the CSV that contains the labels and save it to a df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLfblW9Ji6SJ",
        "outputId": "dcb69c70-364e-418d-84ad-8ad8cf83ec1b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "id                   int64\n",
              "recipient           object\n",
              "recipientaddress    object\n",
              "invoicenos          object\n",
              "invoicedate         object\n",
              "duedate             object\n",
              "Balance             object\n",
              "dtype: object"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "labeldf = pd.read_csv(\"./files/data.csv\")\n",
        "labeldf.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amLlkuGojlS4"
      },
      "source": [
        "## Pick the images first and extract data from the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "x3NL4oZ8jpdI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image 0\n",
            "image 1\n"
          ]
        }
      ],
      "source": [
        "import pytesseract\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "\n",
        "\n",
        "# Define the directory path where your invoice images are located\n",
        "directory_path = \"./files/\"  \n",
        "\n",
        "# Define a list of image file extensions (add more if needed)\n",
        "image_extensions = ['*.jpg', '*.jpeg', '*.png']\n",
        "\n",
        "# Initialize lists to store image data and labels\n",
        "image_data = []\n",
        "\n",
        "i = 0\n",
        "\n",
        "for extension in image_extensions:\n",
        "    pattern = os.path.join(directory_path, extension)\n",
        "    image_files = glob.glob(pattern)\n",
        "    for img_path in image_files:\n",
        "      full_name = os.path.basename(img_path)\n",
        "      file_name = os.path.splitext(full_name)\n",
        "      # Load image\n",
        "      img = cv2.imread(img_path)\n",
        "\n",
        "      # Convert image to grayscale\n",
        "      gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "      # Apply threshold to convert to binary image\n",
        "      threshold_img = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
        "\n",
        "      # Pass the image through pytesseract\n",
        "      text = pytesseract.image_to_string(threshold_img)\n",
        "\n",
        "      name = int(file_name[0])\n",
        "      # Print the extracted text\n",
        "      image_data.append([name,text])\n",
        "      i = i + 1\n",
        "      print('image', i)\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNT32VIgppNq"
      },
      "source": [
        "## Save to a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8_kjFKehprx5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>imageid</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>Invoice\\n\\nStanford Plumbing &amp; Heating\\nbeotts...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Stanford Plumbing &amp; Heating\\n128 Madison drive...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   imageid                                               text\n",
              "0        2  Invoice\\n\\nStanford Plumbing & Heating\\nbeotts...\n",
              "1        1  Stanford Plumbing & Heating\\n128 Madison drive..."
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "textdf = pd.DataFrame(image_data,columns=['imageid','text'])  # Store your image data here\n",
        "textdf.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-CJS7Uh0spl"
      },
      "source": [
        "## Pick up the CSV and merge it with the text dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rZGHhefe00yd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "imageid              int64\n",
              "text                object\n",
              "recipient           object\n",
              "recipientaddress    object\n",
              "invoicenos          object\n",
              "invoicedate         object\n",
              "duedate             object\n",
              "Balance             object\n",
              "dtype: object"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labelled_text = textdf.merge(labeldf, left_on='imageid', right_on='id', how='outer')\n",
        "labelled_text = labelled_text.drop(columns=['id'])\n",
        "\n",
        "labelled_text.to_csv(\"./files/extracted_text.csv\", index=False)\n",
        "# Drop unfilled data\n",
        "labelled_text.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yShFFDS716hj"
      },
      "source": [
        "# Model implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz80FYqn5Ile"
      },
      "source": [
        "## importing the csv data for the tokenized photos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pynOwGGOEGNE"
      },
      "outputs": [],
      "source": [
        "# Data of different pages that constitute my training data\n",
        "imported = pd.read_csv(\"./files/extracted_text.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>imageid</th>\n",
              "      <th>text</th>\n",
              "      <th>recipient</th>\n",
              "      <th>recipientaddress</th>\n",
              "      <th>invoicenos</th>\n",
              "      <th>invoicedate</th>\n",
              "      <th>duedate</th>\n",
              "      <th>Balance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>Invoice\\n\\nStanford Plumbing &amp; Heating\\nbeotts...</td>\n",
              "      <td>Allen Smith</td>\n",
              "      <td>123 Madison drive Seattle, WA</td>\n",
              "      <td>INVO2081</td>\n",
              "      <td>Jun 14,2018</td>\n",
              "      <td>Jun 19,2018</td>\n",
              "      <td>2,688.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Stanford Plumbing &amp; Heating\\n128 Madison drive...</td>\n",
              "      <td>Allen Smith</td>\n",
              "      <td>87 Private st, Seattle, WA</td>\n",
              "      <td>INVO2081</td>\n",
              "      <td>11/11/2018</td>\n",
              "      <td>12/10/2018</td>\n",
              "      <td>2,844.80</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   imageid                                               text    recipient  \\\n",
              "0        2  Invoice\\n\\nStanford Plumbing & Heating\\nbeotts...  Allen Smith   \n",
              "1        1  Stanford Plumbing & Heating\\n128 Madison drive...  Allen Smith   \n",
              "\n",
              "                recipientaddress invoicenos  invoicedate      duedate  \\\n",
              "0  123 Madison drive Seattle, WA   INVO2081  Jun 14,2018  Jun 19,2018   \n",
              "1     87 Private st, Seattle, WA   INVO2081   11/11/2018   12/10/2018   \n",
              "\n",
              "    Balance  \n",
              "0  2,688.00  \n",
              "1  2,844.80  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# imported.drop('tokens', axis=1, inplace=True)\n",
        "# imported.drop('embeddings', axis=1, inplace=True)\n",
        "imported.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dropping NaN rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "imageid             0\n",
            "text                0\n",
            "recipient           0\n",
            "recipientaddress    0\n",
            "invoicenos          0\n",
            "invoicedate         0\n",
            "duedate             0\n",
            "Balance             0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "imported.dropna(inplace=True)\n",
        "\n",
        "print(imported.isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LZlDNtYcYih"
      },
      "source": [
        "## initializing annotation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "n6TyKfB5cakL"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# The function that generates the annotations\n",
        "\n",
        "def annotate_ner(text):\n",
        "    # Process the text with spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Initialize a list to store annotations\n",
        "    annotations = []\n",
        "\n",
        "    # Iterate through entities in the processed text\n",
        "    for ent in doc.ents:\n",
        "        annotations.append({\n",
        "            \"start\": ent.start_char,\n",
        "            \"end\": ent.end_char,\n",
        "            \"label\": ent.label_,\n",
        "            \"text\": ent.text\n",
        "        })\n",
        "\n",
        "    return annotations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIVraxFfmdBv"
      },
      "source": [
        "## Forming annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NJcSOOcceg66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image 1\n",
            "image 2\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "i = 0\n",
        "annot_list = []\n",
        "while i < len(imported):\n",
        "  text = imported['text'][i]\n",
        "\n",
        "  #call the function\n",
        "  annotations = annotate_ner(text)\n",
        "\n",
        "  # save the annotations\n",
        "  annot_list.append([i+1,annotations])\n",
        "  print(f\"image {i+1}\") #display the number of annotated images\n",
        "  i = i+1\n",
        "\n",
        "annotations = pd.DataFrame(annot_list,columns=['id','annotations'])\n",
        "annotations.to_csv(\"./files/annotations.csv\" ,index = False)\n",
        "annotations.to_json(\"./files/annotations.json\" ,index = False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>annotations</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>[{'start': 9, 'end': 36, 'label': 'ORG', 'text...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>[{'start': 0, 'end': 27, 'label': 'ORG', 'text...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                        annotations\n",
              "0   1  [{'start': 9, 'end': 36, 'label': 'ORG', 'text...\n",
              "1   2  [{'start': 0, 'end': 27, 'label': 'ORG', 'text..."
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "annotations.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ORG Label Text: Stanford Plumbing & Heating\n",
            "ORG Label Text: ety plow ana\n",
            "ORG Label Text: Stanford Plumbing & Heating\n",
            "ORG Label Text: Nest\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "json_file_path = './files/annotations.json'\n",
        "\n",
        "# Open and read the JSON file\n",
        "with open(json_file_path, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "# Initialize a list to store ORG labels and text\n",
        "org_labels_text = []\n",
        "\n",
        "# Iterate through the annotations\n",
        "for annotations_list in data['annotations'].values():\n",
        "    for annotation in annotations_list:\n",
        "        if annotation['label'] == 'ORG':\n",
        "            org_labels_text.append(annotation['text'])\n",
        "\n",
        "# Print the collected ORG labels and text\n",
        "for org_text in org_labels_text:\n",
        "    print(f\"ORG Label Text: {org_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "w4Qedmvqg6x-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "imageid              int64\n",
              "text                object\n",
              "recipient           object\n",
              "recipientaddress    object\n",
              "invoicenos          object\n",
              "invoicedate         object\n",
              "duedate             object\n",
              "Balance             object\n",
              "annotations         object\n",
              "dtype: object"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# add the annotations to the df containing the text and the labels\n",
        "imported = imported.merge(annotations, left_on='imageid', right_on='id', how='outer')\n",
        "imported.drop('id', axis=1, inplace=True)\n",
        "imported.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNf5eB6jmgQ-"
      },
      "source": [
        "## spaCy tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "bS79TiOKiolS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image 1\n",
            "image 2\n"
          ]
        }
      ],
      "source": [
        "# Using the spaCy model to tokenize the text\n",
        "spacytokens_list = []\n",
        "i = 0\n",
        "\n",
        "while i < len(imported):\n",
        "  # Text to be tokenized\n",
        "  text = imported['text'][i]\n",
        "\n",
        "  # tokens\n",
        "  doc = nlp(text)\n",
        "\n",
        "  # Access tokens in the processed text\n",
        "  tokens = [token.text for token in doc]\n",
        "\n",
        "  # Print the annotations\n",
        "  spacytokens_list.append([i+1,tokens])\n",
        "  print(f\"image {i+1}\")\n",
        "  i = i+1\n",
        "\n",
        "spacytokens = pd.DataFrame(spacytokens_list,columns=['id','spacytokens'])\n",
        "spacytokens.to_json(\"./files/tokens.json\" ,index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BqNlhiq9kn6h"
      },
      "outputs": [],
      "source": [
        "# Adding the tokens to the df and saving the data to a csv\n",
        "imported = imported.merge(spacytokens, left_on='imageid', right_on='id', how='outer')\n",
        "imported.drop('id', axis=1, inplace=True)\n",
        "imported.to_csv('./files/tokenized_text.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMkBA2fAmjRH"
      },
      "source": [
        "## Entity creations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "SHDRBulgmDqg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image 1\n",
            "image 2\n"
          ]
        }
      ],
      "source": [
        "# Separating the entities and the labels from the data that passed throught the model\n",
        "\n",
        "entity_list = []\n",
        "i=0\n",
        "while i < len(imported):\n",
        "  tokenized = imported['spacytokens'][i]\n",
        "  string = \" \".join(tokenized)\n",
        "  # Process the tokenized text with spaCy\n",
        "  doc = nlp(string)\n",
        "\n",
        "  # Access named entities in the processed text\n",
        "  for ent in doc.ents:\n",
        "    entity_list.append([i+1,ent.text,ent.label_])\n",
        "\n",
        "  print(f'image {i+1}')\n",
        "  i=i+1\n",
        "\n",
        "spacyentities = pd.DataFrame(entity_list,columns=['id','entity_text','entity_label'])\n",
        "spacyentities.to_csv(\"./files/entities.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grouping the labels and entities from the results according to the id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "PI0wDrGx3p5c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Group by 'ID' and aggregate 'Label' and 'Title' as a list of dictionaries\n",
        "result = spacyentities.groupby('id').apply(lambda x: x[['entity_label', 'entity_text']].to_dict(orient='records')).reset_index(name='Data')\n",
        "\n",
        "# Initialize an empty list to store the output dictionaries\n",
        "output_list = []\n",
        "\n",
        "# Create dictionaries for each 'ID' containing arrays of labels and titles\n",
        "for index, row in result.iterrows():\n",
        "    id_dict = {'id': row['id'], 'Data': row['Data']}\n",
        "    output_list.append(id_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhpCOsPPzZ2I",
        "outputId": "f7c6404d-6abc-45f6-cbc0-0627c6908496"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "{'id': 1, 'Data': [{'entity_label': 'ORG', 'entity_text': 'Stanford Plumbing & Heating'}, {'entity_label': 'CARDINAL', 'entity_text': '128'}, {'entity_label': 'PERSON', 'entity_text': 'Madison'}, {'entity_label': 'GPE', 'entity_text': 'WA'}, {'entity_label': 'DATE', 'entity_text': '72290'}, {'entity_label': 'PERSON', 'entity_text': 'Allen Sith'}, {'entity_label': 'CARDINAL', 'entity_text': '87'}, {'entity_label': 'GPE', 'entity_text': 'Seattle'}, {'entity_label': 'DATE', 'entity_text': '9002 - 1898'}, {'entity_label': 'ORG', 'entity_text': 'nea newtehen'}, {'entity_label': 'ORG', 'entity_text': 'Toto'}, {'entity_label': 'CARDINAL', 'entity_text': '20'}, {'entity_label': 'LAW', 'entity_text': 'PayPal feb Getanforplumbing'}, {'entity_label': 'PERSON', 'entity_text': 'Balance Due'}, {'entity_label': 'MONEY', 'entity_text': '2,808.90'}]}\n",
            "{'id': 2, 'Data': [{'entity_label': 'ORG', 'entity_text': 'Stanford Plumbing & Heating \\n beottsanforepLamarg cm'}, {'entity_label': 'CARDINAL', 'entity_text': '12s'}, {'entity_label': 'CARDINAL', 'entity_text': '20'}, {'entity_label': 'DATE', 'entity_text': '2018'}, {'entity_label': 'DATE', 'entity_text': '2018'}, {'entity_label': 'PERSON', 'entity_text': 'Allen Smith'}, {'entity_label': 'DATE', 'entity_text': '1920'}, {'entity_label': 'CARDINAL', 'entity_text': 'suc'}, {'entity_label': 'CARDINAL', 'entity_text': '9'}, {'entity_label': 'ORG', 'entity_text': 'ety plow ana'}, {'entity_label': 'CARDINAL', 'entity_text': '54'}, {'entity_label': 'PERSON', 'entity_text': '¢~ ens'}, {'entity_label': 'CARDINAL', 'entity_text': '28'}, {'entity_label': 'MONEY', 'entity_text': '1 sue'}, {'entity_label': 'NORP', 'entity_text': 'Nese'}, {'entity_label': 'CARDINAL', 'entity_text': '1'}, {'entity_label': 'CARDINAL', 'entity_text': '222 088'}, {'entity_label': 'DATE', 'entity_text': 'Greenscar 3th'}, {'entity_label': 'CARDINAL', 'entity_text': '1'}, {'entity_label': 'GPE', 'entity_text': 'Brera'}, {'entity_label': 'MONEY', 'entity_text': '2,688.00'}, {'entity_label': 'PRODUCT', 'entity_text': 'Notes'}, {'entity_label': 'DATE', 'entity_text': '20 day'}, {'entity_label': 'PERSON', 'entity_text': 'Inark'}]}\n"
          ]
        }
      ],
      "source": [
        "print(len(output_list))\n",
        "# Now, output_list contains dictionaries with dimensions 1x1\n",
        "for item in output_list:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ChatGPT including"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Include chatgpt in deduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# key = \"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\"\n",
        "# import openai\n",
        "\n",
        "# # Set your API key\n",
        "# openai.api_key = key\n",
        "\n",
        "# # Test the API connection by making a simple request with an engine specified\n",
        "# try:\n",
        "#     response = openai.Completion.create(\n",
        "#         prompt=\"Test connection\",\n",
        "#         engine=\"text-davinci-002\"  # Specify the engine or model you want to use\n",
        "#     )\n",
        "#     if response.choices:\n",
        "#         print(\"API connection test successful.\")\n",
        "#         print(\"Response:\", response.choices[0].text)\n",
        "#     else:\n",
        "#         print(\"API request was successful, but the response is empty.\")\n",
        "# except Exception as e:\n",
        "#     print(\"API connection test failed. Error:\", str(e))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os, openai, shutil, json\n",
        "# from llama_index import Document, VectorStoreIndex, LLMPredictor, Prompt\n",
        "# from langchain.chat_models import ChatOpenAI\n",
        "# from dotenv import load_dotenv\n",
        "# from typing import Any, List\n",
        "\n",
        "# load_dotenv()\n",
        "\n",
        "# os.environ['USE_TORCH'] = '1'\n",
        "# os.environ['OPENAI_API_KEY'] = str(os.getenv('OPENAI_API_KEY'))\n",
        "# openai.api_key = str(os.getenv('OPENAI_API_KEY'))\n",
        "# openai.api_endpoint = \"https://api.openai.com/v1\"\n",
        "\n",
        "# TEMPLATE = (\n",
        "#     \"You are a helpful assistant that extracts specific information from unstructured data. That data is provided below\\n\"\n",
        "#     \"---------------------\\n\"\n",
        "#     \"{context_str}\"\n",
        "#     \"\\n---------------------\\n\"\n",
        "#     \"Given this information, please answer the question with precision: {query_str}\\n\"\n",
        "# )\n",
        "# QA_TEMPLATE = Prompt(TEMPLATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def extract_data(text:str, questions:List[str]) -> str:\n",
        "#     document = Document(text=text)\n",
        "\n",
        "#     llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name='gpt-4'))\n",
        "#     index = VectorStoreIndex.from_documents([document], llm_predictor=llm_predictor)\n",
        "\n",
        "#     query_engine = index.as_query_engine(text_qa_template=QA_TEMPLATE)\n",
        "\n",
        "#     result_string = \"\"\n",
        "#     for q in questions:\n",
        "#         result_string += '---------------\\n'\n",
        "#         # result_string += 'QUESTION:\\n'\n",
        "#         # result_string += q + '\\n'\n",
        "#         # result_string += 'ANSWER:\\n'\n",
        "#         result_string += query_engine.query(q).response + '\\n'\n",
        "\n",
        "#     return result_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def lambda_handler(text):\n",
        "#     questions_string = [\n",
        "#         'what is the name of the recipient company',\n",
        "#         'what is the address of the recipient company',\n",
        "#         'what is the invoice number',\n",
        "#         'what is the invoice date',\n",
        "#         'what is the due date of the invoice',\n",
        "#         'what is the due balance to be paid',\n",
        "#     ]\n",
        "#     questions = questions_string if isinstance(questions_string, list) else questions_string.split(',')\n",
        "\n",
        "#     result = extract_data(text, questions)\n",
        "\n",
        "#     return(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# lambda_handler(newimported['text'][0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
