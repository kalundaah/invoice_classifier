{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7bk-v0gftmm"
      },
      "source": [
        "# PDF PROCESSING MODEL.\n",
        "\n",
        "This model is generated to extract data from invoice pdfs. This data should entail the recipient name, address etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQH-y2mw4rb4"
      },
      "source": [
        "# Collecting the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjpL2CXwgG6I"
      },
      "source": [
        "## Installing dependencies and importing them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6H_1P2Gfr2C",
        "outputId": "dca61fcd-9907-4592-ce15-9252da8409cf"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "# !pip install pandas\n",
        "# !pip install numpy\n",
        "# !pip install opencv-python\n",
        "# !pip install pytesseract\n",
        "# !dnf install tesseract-ocr\n",
        "# !dnf install poppler-utils\n",
        "# !pip install scikit-learn\n",
        "# !pip install -U spacy\n",
        "# !python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZS4OFIlhgKj"
      },
      "source": [
        "## Pick the CSV that contains the labels and save it to a df\n"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 5,
=======
      "execution_count": 38,
>>>>>>> parent of e2d7dc0 (for numerous files)
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLfblW9Ji6SJ",
        "outputId": "dcb69c70-364e-418d-84ad-8ad8cf83ec1b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "id                   int64\n",
              "recipient           object\n",
              "recipientaddress    object\n",
              "invoicenos          object\n",
              "invoicedate         object\n",
              "duedate             object\n",
              "Balance             object\n",
              "dtype: object"
            ]
          },
<<<<<<< HEAD
          "execution_count": 5,
=======
          "execution_count": 38,
>>>>>>> parent of e2d7dc0 (for numerous files)
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "labeldf = pd.read_csv(\"./files/data.csv\")\n",
        "labeldf.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amLlkuGojlS4"
      },
      "source": [
        "## Pick the images first and extract data from the images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### For recurrent data"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 6,
=======
      "execution_count": 39,
>>>>>>> parent of e2d7dc0 (for numerous files)
      "metadata": {
        "id": "x3NL4oZ8jpdI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image 1\n",
<<<<<<< HEAD
            "image 2\n",
            "image 3\n",
            "image 4\n",
            "image 5\n",
            "image 6\n",
            "image 7\n",
            "image 8\n",
            "image 9\n",
            "image 10\n",
            "image 11\n",
            "image 12\n",
            "image 13\n",
            "image 14\n",
            "image 15\n",
            "image 16\n",
            "image 17\n",
            "image 18\n",
            "image 19\n",
            "image 20\n",
            "image 21\n",
            "image 22\n",
            "image 23\n",
            "image 24\n",
            "image 25\n",
            "image 26\n",
            "image 27\n",
            "image 28\n",
            "image 29\n",
            "image 30\n",
            "image 31\n",
            "image 32\n",
            "image 33\n",
            "image 34\n",
            "image 35\n",
            "image 36\n",
            "image 37\n",
            "image 38\n",
            "image 39\n",
            "image 40\n",
            "image 41\n",
            "image 42\n",
            "image 43\n",
            "image 44\n",
            "image 45\n",
            "image 46\n",
            "image 47\n",
            "image 48\n",
            "image 49\n",
            "image 50\n",
            "image 51\n",
            "image 52\n",
            "image 53\n",
            "image 54\n",
            "image 55\n",
            "image 56\n",
            "image 57\n",
            "image 58\n",
            "image 59\n",
            "image 60\n",
            "image 61\n",
            "image 62\n",
            "image 63\n",
            "image 64\n",
            "image 65\n",
            "image 66\n",
            "image 67\n",
            "image 68\n",
            "image 69\n",
            "image 70\n",
            "image 71\n",
            "image 72\n",
            "image 73\n",
            "image 74\n",
            "image 75\n",
            "image 76\n",
            "image 77\n",
            "image 78\n",
            "image 79\n",
            "image 80\n",
            "image 81\n",
            "image 82\n",
            "image 83\n",
            "image 84\n",
            "image 85\n",
            "image 86\n",
            "image 87\n",
            "image 88\n",
            "image 89\n",
            "image 90\n",
            "image 91\n",
            "image 92\n",
            "image 93\n",
            "image 94\n",
            "image 95\n",
            "image 96\n",
            "image 97\n",
            "image 98\n",
            "image 99\n",
            "image 100\n",
            "image 101\n",
            "image 102\n",
            "image 103\n",
            "image 104\n",
            "image 105\n",
            "image 106\n",
            "image 107\n",
            "image 108\n",
            "image 109\n",
            "image 110\n",
            "image 111\n",
            "image 112\n",
            "image 113\n",
            "image 114\n",
            "image 115\n",
            "image 116\n",
            "image 117\n",
            "image 118\n",
            "image 119\n",
            "image 120\n",
            "image 121\n",
            "image 122\n",
            "image 123\n",
            "image 124\n",
            "image 125\n",
            "image 126\n",
            "image 127\n",
            "image 128\n",
            "image 129\n",
            "image 130\n",
            "image 131\n",
            "image 132\n",
            "image 133\n",
            "image 134\n",
            "image 135\n",
            "image 136\n",
            "image 137\n",
            "image 138\n",
            "image 139\n",
            "image 140\n",
            "image 141\n",
            "image 142\n",
            "image 143\n",
            "image 144\n",
            "image 145\n",
            "image 146\n",
            "image 147\n",
            "image 148\n",
            "image 149\n",
            "image 150\n",
            "image 151\n",
            "image 152\n",
            "image 153\n",
            "image 154\n",
            "image 155\n",
            "image 156\n",
            "image 157\n",
            "image 158\n",
            "image 159\n",
            "image 160\n",
            "image 161\n",
            "image 162\n",
            "image 163\n",
            "image 164\n",
            "image 165\n",
            "image 166\n",
            "image 167\n",
            "image 168\n",
            "image 169\n",
            "image 170\n",
            "image 171\n",
            "image 172\n",
            "image 173\n",
            "image 174\n",
            "image 175\n",
            "image 176\n",
            "image 177\n",
            "image 178\n",
            "image 179\n",
            "image 180\n",
            "image 181\n",
            "image 182\n",
            "image 183\n",
            "image 184\n",
            "image 185\n",
            "image 186\n",
            "image 187\n",
            "image 188\n",
            "image 189\n",
            "image 190\n",
            "image 191\n",
            "image 192\n",
            "image 193\n",
            "image 194\n",
            "image 195\n",
            "image 196\n",
            "image 197\n",
            "image 198\n",
            "image 199\n",
            "image 200\n"
=======
            "image 2\n"
>>>>>>> parent of e2d7dc0 (for numerous files)
          ]
        }
      ],
      "source": [
        "import pytesseract\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "\n",
        "\n",
        "# Define the directory path where your invoice images are located\n",
        "directory_path = \"./files/\"  \n",
        "\n",
        "# Define a list of image file extensions (add more if needed)\n",
        "image_extensions = ['*.jpg', '*.jpeg', '*.png']\n",
        "\n",
        "# Initialize lists to store image data and labels\n",
        "image_data = []\n",
        "\n",
        "i = 0\n",
<<<<<<< HEAD
        "while i != 200:\n",
        "  for extension in image_extensions:\n",
        "      pattern = os.path.join(directory_path, extension)\n",
        "      image_files = glob.glob(pattern)\n",
        "      for img_path in image_files:\n",
        "        full_name = os.path.basename(img_path)\n",
        "        file_name = os.path.splitext(full_name)\n",
        "        # Load image\n",
        "        img = cv2.imread(img_path)\n",
=======
        "# while i != 1300:\n",
        "#   for extension in image_extensions:\n",
        "#       pattern = os.path.join(directory_path, extension)\n",
        "#       image_files = glob.glob(pattern)\n",
        "#       for img_path in image_files:\n",
        "#         full_name = os.path.basename(img_path)\n",
        "#         file_name = os.path.splitext(full_name)\n",
        "#         # Load image\n",
        "#         img = cv2.imread(img_path)\n",
>>>>>>> parent of e2d7dc0 (for numerous files)
        "\n",
        "#         # Convert image to grayscale\n",
        "#         gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "#         # Apply threshold to convert to binary image\n",
        "#         threshold_img = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
        "\n",
        "#         # Pass the image through pytesseract\n",
        "#         text = pytesseract.image_to_string(threshold_img)\n",
        "\n",
<<<<<<< HEAD
        "        name = int(file_name[0])\n",
        "        # Print the extracted text\n",
        "        image_data.append([name,text])\n",
        "        i = i + 1\n",
        "        print('image', i)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### For only the required data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# i = 0\n",
=======
        "#         name = int(file_name[0])\n",
        "#         # Print the extracted text\n",
        "#         image_data.append([name,text])\n",
        "#         i = i + 1\n",
        "#         print('image', i)\n",
>>>>>>> parent of e2d7dc0 (for numerous files)
        "\n",
        "for extension in image_extensions:\n",
        "    pattern = os.path.join(directory_path, extension)\n",
        "    image_files = glob.glob(pattern)\n",
        "    for img_path in image_files:\n",
        "      full_name = os.path.basename(img_path)\n",
        "      file_name = os.path.splitext(full_name)\n",
        "      # Load image\n",
        "      img = cv2.imread(img_path)\n",
        "\n",
        "      # Convert image to grayscale\n",
        "      gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "      # Apply threshold to convert to binary image\n",
        "      threshold_img = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
        "\n",
        "      # Pass the image through pytesseract\n",
        "      text = pytesseract.image_to_string(threshold_img)\n",
        "\n",
        "      name = int(file_name[0])\n",
        "      # Print the extracted text\n",
        "      image_data.append([name,text])\n",
        "      i = i + 1\n",
        "      print('image', i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNT32VIgppNq"
      },
      "source": [
        "## Save to a dataframe"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 7,
=======
      "execution_count": 40,
>>>>>>> parent of e2d7dc0 (for numerous files)
      "metadata": {
        "id": "8_kjFKehprx5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>imageid</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>Invoice\\n\\nStanford Plumbing &amp; Heating\\nbeotts...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Stanford Plumbing &amp; Heating\\n128 Madison drive...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   imageid                                               text\n",
              "0        2  Invoice\\n\\nStanford Plumbing & Heating\\nbeotts...\n",
              "1        1  Stanford Plumbing & Heating\\n128 Madison drive..."
            ]
          },
<<<<<<< HEAD
          "execution_count": 7,
=======
          "execution_count": 40,
>>>>>>> parent of e2d7dc0 (for numerous files)
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "textdf = pd.DataFrame(image_data,columns=['imageid','text'])  # Store your image data here\n",
        "textdf.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-CJS7Uh0spl"
      },
      "source": [
        "## Pick up the CSV and merge it with the text dataframe"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 8,
=======
      "execution_count": 41,
>>>>>>> parent of e2d7dc0 (for numerous files)
      "metadata": {
        "id": "rZGHhefe00yd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "imageid              int64\n",
              "text                object\n",
              "recipient           object\n",
              "recipientaddress    object\n",
              "invoicenos          object\n",
              "invoicedate         object\n",
              "duedate             object\n",
              "Balance             object\n",
              "dtype: object"
            ]
          },
<<<<<<< HEAD
          "execution_count": 8,
=======
          "execution_count": 41,
>>>>>>> parent of e2d7dc0 (for numerous files)
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labelled_text = textdf.merge(labeldf, left_on='imageid', right_on='id', how='outer')\n",
        "labelled_text = labelled_text.drop(columns=['id'])\n",
        "\n",
        "labelled_text.to_csv(\"./files/extracted_text.csv\", index=False)\n",
        "# Drop unfilled data\n",
        "labelled_text.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yShFFDS716hj"
      },
      "source": [
        "# Model implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz80FYqn5Ile"
      },
      "source": [
        "## importing the csv data for the tokenized photos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "pynOwGGOEGNE"
      },
      "outputs": [],
      "source": [
        "# Data of different pages that constitute my training data\n",
        "imported = pd.read_csv(\"./files/extracted_text.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>imageid</th>\n",
              "      <th>text</th>\n",
              "      <th>recipient</th>\n",
              "      <th>recipientaddress</th>\n",
              "      <th>invoicenos</th>\n",
              "      <th>invoicedate</th>\n",
              "      <th>duedate</th>\n",
              "      <th>Balance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>Invoice\\n\\nStanford Plumbing &amp; Heating\\nbeotts...</td>\n",
              "      <td>Allen Smith</td>\n",
              "      <td>123 Madison drive Seattle, WA</td>\n",
              "      <td>INVO2081</td>\n",
              "      <td>Jun 14,2018</td>\n",
              "      <td>Jun 19,2018</td>\n",
              "      <td>2,688.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Stanford Plumbing &amp; Heating\\n128 Madison drive...</td>\n",
              "      <td>Allen Smith</td>\n",
              "      <td>87 Private st, Seattle, WA</td>\n",
              "      <td>INVO2081</td>\n",
              "      <td>11/11/2018</td>\n",
              "      <td>12/10/2018</td>\n",
              "      <td>2,844.80</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   imageid                                               text    recipient  \\\n",
              "0        2  Invoice\\n\\nStanford Plumbing & Heating\\nbeotts...  Allen Smith   \n",
              "1        1  Stanford Plumbing & Heating\\n128 Madison drive...  Allen Smith   \n",
              "\n",
              "                recipientaddress invoicenos  invoicedate      duedate  \\\n",
              "0  123 Madison drive Seattle, WA   INVO2081  Jun 14,2018  Jun 19,2018   \n",
              "1     87 Private st, Seattle, WA   INVO2081   11/11/2018   12/10/2018   \n",
              "\n",
              "    Balance  \n",
              "0  2,688.00  \n",
              "1  2,844.80  "
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# imported.drop('tokens', axis=1, inplace=True)\n",
        "# imported.drop('embeddings', axis=1, inplace=True)\n",
        "imported.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dropping NaN rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "imageid             0\n",
            "text                0\n",
            "recipient           0\n",
            "recipientaddress    0\n",
            "invoicenos          0\n",
            "invoicedate         0\n",
            "duedate             0\n",
            "Balance             0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "imported.dropna(inplace=True)\n",
        "\n",
        "print(imported.isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LZlDNtYcYih"
      },
      "source": [
        "## initializing annotation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "n6TyKfB5cakL"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# The function that generates the annotations\n",
        "\n",
        "def annotate_ner(text):\n",
        "    # Process the text with spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Initialize a list to store annotations\n",
        "    annotations = []\n",
        "\n",
        "    # Iterate through entities in the processed text\n",
        "    for ent in doc.ents:\n",
        "        annotations.append({\n",
        "            \"start\": ent.start_char,\n",
        "            \"end\": ent.end_char,\n",
        "            \"label\": ent.label_,\n",
        "            \"text\": ent.text\n",
        "        })\n",
        "\n",
        "    return annotations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIVraxFfmdBv"
      },
      "source": [
        "## Forming annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "NJcSOOcceg66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image 1\n",
            "image 2\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "i = 0\n",
        "annot_list = []\n",
        "while i < len(imported):\n",
        "  text = imported['text'][i]\n",
        "\n",
        "  #call the function\n",
        "  annotations = annotate_ner(text)\n",
        "\n",
        "  # save the annotations\n",
        "  annot_list.append([i+1,annotations])\n",
        "  print(f\"image {i+1}\") #display the number of annotated images\n",
        "  i = i+1\n",
        "\n",
        "annotations = pd.DataFrame(annot_list,columns=['id','annotations'])\n",
        "annotations.to_csv(\"./files/annotations.csv\" ,index = False)\n",
        "annotations.to_json(\"./files/annotations.json\" ,index = False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>annotations</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>[{'start': 9, 'end': 36, 'label': 'ORG', 'text...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>[{'start': 0, 'end': 27, 'label': 'ORG', 'text...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                        annotations\n",
              "0   1  [{'start': 9, 'end': 36, 'label': 'ORG', 'text...\n",
              "1   2  [{'start': 0, 'end': 27, 'label': 'ORG', 'text..."
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "annotations.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reading from Json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ORG Label Text: Stanford Plumbing & Heating\n",
            "ORG Label Text: ety plow ana\n",
            "ORG Label Text: Stanford Plumbing & Heating\n",
            "ORG Label Text: Nest\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "json_file_path = './files/annotations.json'\n",
        "\n",
        "# Open and read the JSON file\n",
        "with open(json_file_path, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "# Initialize a list to store ORG labels and text\n",
        "org_labels_text = []\n",
        "\n",
        "# Iterate through the annotations\n",
        "for annotations_list in data['annotations'].values():\n",
        "    for annotation in annotations_list:\n",
        "        if annotation['label'] == 'ORG':\n",
        "            org_labels_text.append(annotation['text'])\n",
        "\n",
        "# Print the collected ORG labels and text\n",
        "for org_text in org_labels_text:\n",
        "    print(f\"ORG Label Text: {org_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "w4Qedmvqg6x-"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'Series' object has no attribute 'merge'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_23278/2094689884.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# add the annotations to the df containing the text and the labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimported\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotations_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imageid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'outer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimported\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimported\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'imageid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimported\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6200\u001b[0m         ):\n\u001b[1;32m   6201\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6202\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'merge'"
          ]
        }
      ],
      "source": [
        "# add the annotations to the df containing the text and the labels\n",
        "imported = imported.merge(annotations, left_on='imageid', right_on='id', how='outer')\n",
        "imported.drop('id', axis=1, inplace=True)\n",
        "imported = imported['imageid'].astype(str)\n",
        "imported.dtypes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNf5eB6jmgQ-"
      },
      "source": [
        "## spaCy tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "bS79TiOKiolS"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'text'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32m/home/kalundaah/Documents/GitHub/invoice_classifier/official.ipynb Cell 28\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kalundaah/Documents/GitHub/invoice_classifier/official.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kalundaah/Documents/GitHub/invoice_classifier/official.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mwhile\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(imported):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kalundaah/Documents/GitHub/invoice_classifier/official.ipynb#X35sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m   \u001b[39m# Text to be tokenized\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kalundaah/Documents/GitHub/invoice_classifier/official.ipynb#X35sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m   text \u001b[39m=\u001b[39m imported[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m][i]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kalundaah/Documents/GitHub/invoice_classifier/official.ipynb#X35sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m   \u001b[39m# tokens\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kalundaah/Documents/GitHub/invoice_classifier/official.ipynb#X35sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m   doc \u001b[39m=\u001b[39m nlp(text)\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/series.py:1040\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1037\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m   1039\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1040\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m   1042\u001b[0m \u001b[39m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[39m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m is_iterator(key):\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/series.py:1156\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   1155\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1156\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[1;32m   1158\u001b[0m \u001b[39mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[loc]\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/indexes/range.py:418\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m--> 418\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n\u001b[1;32m    419\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m    420\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n",
            "\u001b[0;31mKeyError\u001b[0m: 'text'"
          ]
        }
      ],
      "source": [
        "# Using the spaCy model to tokenize the text\n",
        "spacytokens_list = []\n",
        "i = 0\n",
        "\n",
        "while i < len(imported):\n",
        "  # Text to be tokenized\n",
        "  text = imported['text'][i]\n",
        "\n",
        "  # tokens\n",
        "  doc = nlp(text)\n",
        "\n",
        "  # Access tokens in the processed text\n",
        "  tokens = [token.text for token in doc]\n",
        "\n",
        "  # Print the annotations\n",
        "  spacytokens_list.append([i+1,tokens])\n",
        "  print(f\"image {i+1}\")\n",
        "  i = i+1\n",
        "\n",
        "\n",
        "spacytokens = pd.DataFrame(spacytokens_list,columns=[id,'spacytokens'])\n",
        "spacytokens.to_json(\"./files/tokens.json\" ,index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqNlhiq9kn6h"
      },
      "outputs": [],
      "source": [
        "# Adding the tokens to the df and saving the data to a csv\n",
        "imported = imported.merge(spacytokens, left_on='imageid', right_on='id', how='outer')\n",
        "imported.drop('id', axis=1, inplace=True)\n",
        "imported.to_csv('./files/tokenized_text.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMkBA2fAmjRH"
      },
      "source": [
        "## Entity creations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHDRBulgmDqg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image 1\n",
            "image 2\n"
          ]
        }
      ],
      "source": [
        "# Separating the entities and the labels from the data that passed throught the model\n",
        "\n",
        "entity_list = []\n",
        "i=0\n",
        "while i < len(imported):\n",
        "  tokenized = imported['spacytokens'][i]\n",
        "  string = \" \".join(tokenized)\n",
        "  # Process the tokenized text with spaCy\n",
        "  doc = nlp(string)\n",
        "\n",
        "  # Access named entities in the processed text\n",
        "  for ent in doc.ents:\n",
        "    entity_list.append([i+1,ent.text,ent.label_])\n",
        "\n",
        "  print(f'image {i+1}')\n",
        "  i=i+1\n",
        "\n",
        "spacyentities = pd.DataFrame(entity_list,columns=['id','entity_text','entity_label'])\n",
        "spacyentities.to_csv(\"./files/entities.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grouping the labels and entities from the results according to the id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PI0wDrGx3p5c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Group by 'ID' and aggregate 'Label' and 'Title' as a list of dictionaries\n",
        "result = spacyentities.groupby('id').apply(lambda x: x[['entity_label', 'entity_text']].to_dict(orient='records')).reset_index(name='Data')\n",
        "\n",
        "# Initialize an empty list to store the output dictionaries\n",
        "output_list = []\n",
        "\n",
        "# Create dictionaries for each 'ID' containing arrays of labels and titles\n",
        "for index, row in result.iterrows():\n",
        "    id_dict = {'id': row['id'], 'Data': row['Data']}\n",
        "    output_list.append(id_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhpCOsPPzZ2I",
        "outputId": "f7c6404d-6abc-45f6-cbc0-0627c6908496"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "{'id': 1, 'Data': [{'entity_label': 'ORG', 'entity_text': 'Stanford Plumbing & Heating'}, {'entity_label': 'CARDINAL', 'entity_text': '128'}, {'entity_label': 'PERSON', 'entity_text': 'Madison'}, {'entity_label': 'GPE', 'entity_text': 'WA'}, {'entity_label': 'DATE', 'entity_text': '72290'}, {'entity_label': 'PERSON', 'entity_text': 'Allen Sith'}, {'entity_label': 'CARDINAL', 'entity_text': '87'}, {'entity_label': 'GPE', 'entity_text': 'Seattle'}, {'entity_label': 'DATE', 'entity_text': '9002 - 1898'}, {'entity_label': 'ORG', 'entity_text': 'nea newtehen'}, {'entity_label': 'ORG', 'entity_text': 'Toto'}, {'entity_label': 'CARDINAL', 'entity_text': '20'}, {'entity_label': 'LAW', 'entity_text': 'PayPal feb Getanforplumbing'}, {'entity_label': 'PERSON', 'entity_text': 'Balance Due'}, {'entity_label': 'MONEY', 'entity_text': '2,808.90'}]}\n",
            "{'id': 2, 'Data': [{'entity_label': 'ORG', 'entity_text': 'Stanford Plumbing & Heating \\n beottsanforepLamarg cm'}, {'entity_label': 'CARDINAL', 'entity_text': '12s'}, {'entity_label': 'CARDINAL', 'entity_text': '20'}, {'entity_label': 'DATE', 'entity_text': '2018'}, {'entity_label': 'DATE', 'entity_text': '2018'}, {'entity_label': 'PERSON', 'entity_text': 'Allen Smith'}, {'entity_label': 'DATE', 'entity_text': '1920'}, {'entity_label': 'CARDINAL', 'entity_text': 'suc'}, {'entity_label': 'CARDINAL', 'entity_text': '9'}, {'entity_label': 'ORG', 'entity_text': 'ety plow ana'}, {'entity_label': 'CARDINAL', 'entity_text': '54'}, {'entity_label': 'PERSON', 'entity_text': 'Â¢~ ens'}, {'entity_label': 'CARDINAL', 'entity_text': '28'}, {'entity_label': 'MONEY', 'entity_text': '1 sue'}, {'entity_label': 'NORP', 'entity_text': 'Nese'}, {'entity_label': 'CARDINAL', 'entity_text': '1'}, {'entity_label': 'CARDINAL', 'entity_text': '222 088'}, {'entity_label': 'DATE', 'entity_text': 'Greenscar 3th'}, {'entity_label': 'CARDINAL', 'entity_text': '1'}, {'entity_label': 'GPE', 'entity_text': 'Brera'}, {'entity_label': 'MONEY', 'entity_text': '2,688.00'}, {'entity_label': 'PRODUCT', 'entity_text': 'Notes'}, {'entity_label': 'DATE', 'entity_text': '20 day'}, {'entity_label': 'PERSON', 'entity_text': 'Inark'}]}\n"
          ]
        }
      ],
      "source": [
        "print(len(output_list))\n",
        "# Now, output_list contains dictionaries with dimensions 1x1\n",
        "for item in output_list:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ChatGPT including"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Include chatgpt in deduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# key = \"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\"\n",
        "# import openai\n",
        "\n",
        "# # Set your API key\n",
        "# openai.api_key = key\n",
        "\n",
        "# # Test the API connection by making a simple request with an engine specified\n",
        "# try:\n",
        "#     response = openai.Completion.create(\n",
        "#         prompt=\"Test connection\",\n",
        "#         engine=\"text-davinci-002\"  # Specify the engine or model you want to use\n",
        "#     )\n",
        "#     if response.choices:\n",
        "#         print(\"API connection test successful.\")\n",
        "#         print(\"Response:\", response.choices[0].text)\n",
        "#     else:\n",
        "#         print(\"API request was successful, but the response is empty.\")\n",
        "# except Exception as e:\n",
        "#     print(\"API connection test failed. Error:\", str(e))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os, openai, shutil, json\n",
        "# from llama_index import Document, VectorStoreIndex, LLMPredictor, Prompt\n",
        "# from langchain.chat_models import ChatOpenAI\n",
        "# from dotenv import load_dotenv\n",
        "# from typing import Any, List\n",
        "\n",
        "# load_dotenv()\n",
        "\n",
        "# os.environ['USE_TORCH'] = '1'\n",
        "# os.environ['OPENAI_API_KEY'] = str(os.getenv('OPENAI_API_KEY'))\n",
        "# openai.api_key = str(os.getenv('OPENAI_API_KEY'))\n",
        "# openai.api_endpoint = \"https://api.openai.com/v1\"\n",
        "\n",
        "# TEMPLATE = (\n",
        "#     \"You are a helpful assistant that extracts specific information from unstructured data. That data is provided below\\n\"\n",
        "#     \"---------------------\\n\"\n",
        "#     \"{context_str}\"\n",
        "#     \"\\n---------------------\\n\"\n",
        "#     \"Given this information, please answer the question with precision: {query_str}\\n\"\n",
        "# )\n",
        "# QA_TEMPLATE = Prompt(TEMPLATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def extract_data(text:str, questions:List[str]) -> str:\n",
        "#     document = Document(text=text)\n",
        "\n",
        "#     llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name='gpt-4'))\n",
        "#     index = VectorStoreIndex.from_documents([document], llm_predictor=llm_predictor)\n",
        "\n",
        "#     query_engine = index.as_query_engine(text_qa_template=QA_TEMPLATE)\n",
        "\n",
        "#     result_string = \"\"\n",
        "#     for q in questions:\n",
        "#         result_string += '---------------\\n'\n",
        "#         # result_string += 'QUESTION:\\n'\n",
        "#         # result_string += q + '\\n'\n",
        "#         # result_string += 'ANSWER:\\n'\n",
        "#         result_string += query_engine.query(q).response + '\\n'\n",
        "\n",
        "#     return result_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def lambda_handler(text):\n",
        "#     questions_string = [\n",
        "#         'what is the name of the recipient company',\n",
        "#         'what is the address of the recipient company',\n",
        "#         'what is the invoice number',\n",
        "#         'what is the invoice date',\n",
        "#         'what is the due date of the invoice',\n",
        "#         'what is the due balance to be paid',\n",
        "#     ]\n",
        "#     questions = questions_string if isinstance(questions_string, list) else questions_string.split(',')\n",
        "\n",
        "#     result = extract_data(text, questions)\n",
        "\n",
        "#     return(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# lambda_handler(newimported['text'][0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
